{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "232bec5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-15T01:41:26.381924Z",
     "start_time": "2024-02-15T01:41:25.828087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0\tMoving average score: -1053.60\t\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 211\u001b[0m\n\u001b[0;32m    207\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 185\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mstore(Transition(state, action, action_log_prob, (reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m8\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m, state_)):\n\u001b[1;32m--> 185\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m    187\u001b[0m state \u001b[38;5;241m=\u001b[39m state_\n",
      "Cell \u001b[1;32mIn[31], line 133\u001b[0m, in \u001b[0;36mAgent.update\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m r \u001b[38;5;241m=\u001b[39m (r \u001b[38;5;241m-\u001b[39m r\u001b[38;5;241m.\u001b[39mmean()) \u001b[38;5;241m/\u001b[39m (r\u001b[38;5;241m.\u001b[39mstd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 133\u001b[0m     target_v \u001b[38;5;241m=\u001b[39m r \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m adv \u001b[38;5;241m=\u001b[39m (target_v \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnet(s))\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mppo_epoch):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[31], line 57\u001b[0m, in \u001b[0;36mCriticNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     58\u001b[0m     state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_head(x)\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state_value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Solve the Pendulum-v0 with PPO')\n",
    "parser.add_argument(\n",
    "    '--gamma', type=float, default=0.9, metavar='G', help='discount factor (default: 0.9)')\n",
    "parser.add_argument('--seed', type=int, default=0, metavar='N', help='random seed (default: 0)')\n",
    "parser.add_argument('--render', action='store_true', help='render the environment')\n",
    "parser.add_argument(\n",
    "    '--log-interval',\n",
    "    type=int,\n",
    "    default=10,\n",
    "    metavar='N',\n",
    "    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args(args=['--gamma', '0.9', '--seed', '0'])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "TrainingRecord = namedtuple('TrainingRecord', ['ep', 'reward'])\n",
    "Transition = namedtuple('Transition', ['s', 'a', 'a_log_p', 'r', 's_'])\n",
    "\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.fc = nn.Linear(3, 100)\n",
    "        self.mu_head = nn.Linear(100, 1)\n",
    "        self.sigma_head = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        mu = 2.0 * F.tanh(self.mu_head(x))\n",
    "        sigma = F.softplus(self.sigma_head(x))\n",
    "        return (mu, sigma)\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.fc = nn.Linear(3, 100)\n",
    "        self.v_head = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc(x))\n",
    "        state_value = self.v_head(x)\n",
    "        return state_value\n",
    "\n",
    "\n",
    "class Agent():\n",
    "\n",
    "    clip_param = 0.2\n",
    "    max_grad_norm = 0.5\n",
    "    ppo_epoch = 10\n",
    "    buffer_capacity, batch_size = 1000, 32\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training_step = 0\n",
    "        self.anet = ActorNet().float()\n",
    "        self.cnet = CriticNet().float()\n",
    "        self.buffer = []\n",
    "        self.counter = 0\n",
    "\n",
    "        self.optimizer_a = optim.Adam(self.anet.parameters(), lr=1e-4)\n",
    "        self.optimizer_c = optim.Adam(self.cnet.parameters(), lr=3e-4)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if isinstance(state, tuple):\n",
    "            state = state[0]  # 튜플의 첫 번째 요소만 사용        \n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            (mu, sigma) = self.anet(state)\n",
    "        dist = Normal(mu, sigma)\n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action)\n",
    "        action = action.clamp(-2.0, 2.0)\n",
    "        return action.item(), action_log_prob.item()\n",
    "\n",
    "    def get_value(self, state):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            state_value = self.cnet(state)\n",
    "        return state_value.item()\n",
    "\n",
    "    def save_param(self):\n",
    "        torch.save(self.anet.state_dict(), 'param/ppo_anet_params.pkl')\n",
    "        torch.save(self.cnet.state_dict(), 'param/ppo_cnet_params.pkl')\n",
    "\n",
    "    def store(self, transition):\n",
    "        # 상태 s와 s_에서 첫 번째 요소만 사용\n",
    "        s = transition.s[0] if isinstance(transition.s, tuple) else transition.s\n",
    "        s_ = transition.s_[0] if isinstance(transition.s_, tuple) else transition.s_\n",
    "\n",
    "        # 수정된 상태 s와 s_의 형태 확인\n",
    "        if s.shape == (3,) and s_.shape == (3,):\n",
    "            modified_transition = Transition(s, transition.a, transition.a_log_p, transition.r, s_)\n",
    "            self.buffer.append(modified_transition)\n",
    "        else:\n",
    "            raise ValueError(\"Inconsistent state shape\")\n",
    "        self.counter += 1\n",
    "        return self.counter % self.buffer_capacity == 0\n",
    "\n",
    "    def update(self):\n",
    "        self.training_step += 1\n",
    "\n",
    "\n",
    "        # 상태를 NumPy 배열로 변환\n",
    "        s = np.stack([t.s for t in self.buffer])\n",
    "        a = torch.tensor([t.a for t in self.buffer], dtype=torch.float).view(-1, 1)\n",
    "        r = torch.tensor([t.r for t in self.buffer], dtype=torch.float).view(-1, 1)\n",
    "        s_ = np.stack([t.s_ for t in self.buffer])\n",
    "\n",
    "\n",
    "        old_action_log_probs = torch.tensor(\n",
    "            [t.a_log_p for t in self.buffer], dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        r = (r - r.mean()) / (r.std() + 1e-5)\n",
    "        with torch.no_grad():\n",
    "            target_v = r + args.gamma * self.cnet(s_)\n",
    "\n",
    "        adv = (target_v - self.cnet(s)).detach()\n",
    "\n",
    "        for _ in range(self.ppo_epoch):\n",
    "            for index in BatchSampler(\n",
    "                    SubsetRandomSampler(range(self.buffer_capacity)), self.batch_size, False):\n",
    "\n",
    "                (mu, sigma) = self.anet(s[index])\n",
    "                dist = Normal(mu, sigma)\n",
    "                action_log_probs = dist.log_prob(a[index])\n",
    "                ratio = torch.exp(action_log_probs - old_action_log_probs[index])\n",
    "\n",
    "                surr1 = ratio * adv[index]\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n",
    "                                    1.0 + self.clip_param) * adv[index]\n",
    "                action_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                self.optimizer_a.zero_grad()\n",
    "                action_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.anet.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_a.step()\n",
    "\n",
    "                value_loss = F.smooth_l1_loss(self.cnet(s[index]), target_v[index])\n",
    "                self.optimizer_c.zero_grad()\n",
    "                value_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.cnet.parameters(), self.max_grad_norm)\n",
    "                self.optimizer_c.step()\n",
    "\n",
    "        del self.buffer[:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    env = gym.make('Pendulum-v1')\n",
    "#     env.seed(args.seed)\n",
    "\n",
    "    agent = Agent()\n",
    "\n",
    "    training_records = []\n",
    "    running_reward = -1000\n",
    "    state = env.reset()    \n",
    "    for i_ep in range(1000):\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(200):\n",
    "            action, action_log_prob = agent.select_action(state)\n",
    "            state_, reward, done, _, _ = env.step(np.array([action]))  # 모든 반환 값을 변수에 할당\n",
    "            if args.render:\n",
    "                env.render()\n",
    "            if agent.store(Transition(state, action, action_log_prob, (reward + 8) / 8, state_)):\n",
    "                agent.update()\n",
    "            score += reward\n",
    "            state = state_\n",
    "\n",
    "        running_reward = running_reward * 0.9 + score * 0.1\n",
    "        training_records.append(TrainingRecord(i_ep, running_reward))\n",
    "\n",
    "        if i_ep % args.log_interval == 0:\n",
    "            print('Ep {}\\tMoving average score: {:.2f}\\t'.format(i_ep, running_reward))\n",
    "        if running_reward > -200:\n",
    "            print(\"Solved! Moving average score is now {}!\".format(running_reward))\n",
    "            env.close()\n",
    "            agent.save_param()\n",
    "            with open('log/ppo_training_records.pkl', 'wb') as f:\n",
    "                pickle.dump(training_records, f)\n",
    "            break\n",
    "\n",
    "    plt.plot([r.ep for r in training_records], [r.reward for r in training_records])\n",
    "    plt.title('PPO')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving averaged episode reward')\n",
    "    plt.savefig(\"img/ppo.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48327c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
