{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f12dbb3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T07:48:55.197643Z",
     "start_time": "2022-05-02T07:48:55.179691Z"
    }
   },
   "outputs": [],
   "source": [
    "# 필요한 package들을 import 한다. \n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# gym에서 wrappers option을 설정하면 영상 저장이 가능하다.\n",
    "env = gym.make('CartPole-v1').unwrapped\n",
    "\n",
    "# matplotlib 설정\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# interactive-on, 그때 그때 plot을 갱신하는 option\n",
    "plt.ion()\n",
    "\n",
    "# device 설정\n",
    "# GPU를 사용할 수 있으면 사용하고, 아니면 CPU를 사용한다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fedbe",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d3043d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T07:48:55.546710Z",
     "start_time": "2022-05-02T07:48:55.539729Z"
    }
   },
   "outputs": [],
   "source": [
    "# namedtuple은 key와 index를 통해 값에 접근할 수 있다.\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "# ReplayMemory를 정의\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # deque는 양방향 queue를 의미한다.\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        # Transition을 저장하는 부분이다.\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # memory로부터 batch_size 길이 만큼의 list를 반환한다.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        # memory의 길이를 반환한다.\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41539d",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f51b7e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T07:45:58.955181Z",
     "start_time": "2022-05-02T07:45:58.948200Z"
    }
   },
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super(net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00804c82",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1096941e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T07:45:59.358104Z",
     "start_time": "2022-05-02T07:45:59.339155Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==== Hyperparameters ==== #\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "# ========================== #\n",
    "\n",
    "\n",
    "# gym의 action space에서 action의 가짓수를 얻는다.\n",
    "# n_actions >> 2\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# network\n",
    "policy_net = net(n_actions).to(device)\n",
    "target_net = net(n_actions).to(device)\n",
    "\n",
    "# policy network의 network parameter를 불러온다.   \n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "\n",
    "# Capacity (즉, maximum length) 10000짜리 deque 이다.\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    # Global 변수로 선언한다.\n",
    "    global steps_done\n",
    "    # random.random() >> [0.0, 1.0) 구간의 소수점 숫자를 반환한다.\n",
    "    sample = random.random()\n",
    "    # steps_done이 커짐에 따라 epsilon 값이 줄어든다. \n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "    math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # 기댓값이 더 큰 action을 고르자. \n",
    "            # 바로 예를 들어 설명해보면, 아래의 논리로 코드가 진행된다.\n",
    "            '''\n",
    "            policy_net(state) >> tensor([[0.5598, 0.0144]])\n",
    "            policy_net(state).max(1) >> ('max value', 'max 값의 index')\n",
    "            policy_net(state).max(1)[1] >> index를 선택함.\n",
    "            policy_net(state).max(1)[1].view(1, 1) >> tensor([[0]]) \n",
    "            '''\n",
    "            # 즉, 위 예제의 경우 index 0에 해당하는 action을 선택하는 것이다.\n",
    "            return policy_net(state).max(1)[1].view(1,1)\n",
    "    else:\n",
    "        # tensor([['index']])의 형식으로 random하게 action이 선택된다. \n",
    "        # 즉, 0 이나 1 값이 선택됨.\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, \\\n",
    "        dtype=torch.long)\n",
    "    \n",
    "    \n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개 에피소드의 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08582161",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c1a1634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T07:46:00.136025Z",
     "start_time": "2022-05-02T07:46:00.123059Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # 여기서부터는 memory의 길이 (크기)가 BATCH_SIZE 이상인 경우이다.\n",
    "    # BATCH_SIZE의 크기만큼 sampling을 진행한다.  \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    # Remind) Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "    # 아래의 코드를 통해 batch에는 각 항목 별로 BATCH_SIZE 개수 만큼의 성분이 한번에 묶여 저장된다.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 우선 lambda가 포함된 line의 빠른 이해를 위해 다음의 예제를 보자. \n",
    "    # list(map(lambda x: x ** 2, range(5))) >> [0, 1, 4, 9, 16]\n",
    "    '''\n",
    "    즉, 아래의 line을 통해 BATCH_SIZE 개의 원소를 가진 tensor가 구성된다.  \n",
    "    또한 각 원소는 True와 False 로 구성되어 있다. \n",
    "    batch.next_state는 다음 state 값을 가지고 있는 tensor로 크게 두 부류로 구성된다.\n",
    "    >> None 혹은 torch.Size([1, 3, 40, 90]) 의 형태\n",
    "    '''\n",
    "    # 정리하면 아래의 코드는 batch.next_state에서 None을 갖는 원소를 False로, \n",
    "    # 그렇지 않으면 True를 matching 시키는 line이다.\n",
    "#     non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "#                                           batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    # batch.next_state의 원소들 중 next state가 None이 아닌 원소들의 집합이다. \n",
    "    # torch.Size(['next_state가 None이 아닌 원소의 개수', 3, 40, 90])의 형태\n",
    "#     non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "#                                                 if s is not None])\n",
    "\n",
    "    # 아래 세 변수의 size는 모두 torch.Size([128, 3, 40, 90]) 이다. \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "\n",
    "    # torch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor\n",
    "    # action_batch 에 들어있는 0 혹은 1 값으로 index를 설정하여 결과값에서 가져온다.\n",
    "    # 즉, action_batch 값에 해당하는 결과 값을 불러온다.  \n",
    "    current_q_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 한편, non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "    # 일단 모두 0 값을 갖도록 한다.  \n",
    "#     next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # non_final_mask에서 True 값을 가졌던 원소에만 값을 넣을 것이고, False 였던 원소에게는 0 값을 유지할 것이다. \n",
    "    # target_net(non_final_next_states).max(1)[0].detach() 를 하면, \n",
    "    # True 값을 갖는 원소의 개수만큼 max value 값이 모인다.  \n",
    "    # 이들을 True 값의 index 위치에만 반영시키도록 하자.\n",
    "    # 정리하면 한 state에서 더 큰 action을 선택한 것에 대한 value 값이 담기게 된다.  \n",
    "    max_next_q_values = target_net(next_state_batch).detach().max(1)[0]\n",
    "\n",
    "    # expected Q value를 계산하자.  \n",
    "    target_q_values = (max_next_q_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber Loss 계산\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(current_q_values, target_q_values.unsqueeze(1)) # unsqueeze(): 차원 추가 \n",
    "\n",
    "    # Optimize parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        # 모든 원소를 [ min, max ]의 범위로 clamp\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab92ab",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f972bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T07:48:33.488671Z",
     "start_time": "2022-05-02T07:48:22.592795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "# 중요: state, action, next_state, reward 모두 tensor로 넣는것 \n",
    "\n",
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    env.render()\n",
    "    # env와 state를 초기화 한다.  \n",
    "    env.reset()\n",
    "    state = env.reset()\n",
    "    state = torch.tensor([state], device=device, dtype =torch.float) # state에 차원하나 추가 필요 \n",
    "\n",
    "\n",
    "    \n",
    "    # 여기서 사용한 count()는 from itertools import count 로 import 한 것이다. \n",
    "    # t -> 0, 1, 2, ... 의 순서로 진행된다.  \n",
    "    for t in count():\n",
    "        # state shape >> torch.Size([1, 3, 40, 90]) \n",
    "        # action result >> tensor([[0]]) or tensor([[1]])\n",
    "        \n",
    "        action = select_action(state)\n",
    "\n",
    "        # 선택한 action을 대입하여 reward와 done을 얻어낸다. \n",
    "        # env.step(action.item())의 예시  \n",
    "        # >> (array([-0.008956, -0.160571,  0.005936,  0.302326]), 1.0, False, {})\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        reward = torch.tensor([[reward]], device=device)\n",
    "        next_state = torch.tensor([next_state], device=device, dtype =torch.float)\n",
    "        print(state, action, next_state, reward)\n",
    "\n",
    "\n",
    "        # 얻어낸 transition set을 memory에 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (policy network에서) 최적화 한단계 수행\n",
    "        optimize_model()\n",
    "\n",
    "        # 마찬가지로 done이 True 라면,\n",
    "        if done:\n",
    "            # 하나의 episode가 몇 번 진행 되었는지 counting 하는 line\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            plt.show()\n",
    "            break\n",
    "    \n",
    "    # TARGET_UPDATE 마다 target network의 parameter를 update 한다. \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # (episode 한번에 대한) 전체 for문 1회 종료.\n",
    "\n",
    "# 학습 마무리. \n",
    "print('Complete')\n",
    "\n",
    "env.close()\n",
    "# plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2bb72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a30220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a0c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
