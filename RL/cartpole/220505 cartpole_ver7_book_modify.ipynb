{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f12dbb3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T11:13:51.628694Z",
     "start_time": "2022-05-05T11:13:50.722481Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# device 설정:GPU를 사용할 수 있으면 사용하고, 아니면 CPU를 사용한다.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5be650",
   "metadata": {},
   "source": [
    "# Ver6에서 Ver7 바꾼내용\n",
    "* .data -> .detach() 로 변경\n",
    "* Reactoring: 차원맞추는 구문들을 main함수에서 agent 및 memory class 로 변경\n",
    "    * replay memory 내부에서 zip으로 배치사이즈만큼 뱉어주도록 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fedbe",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d3043d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T11:13:51.788731Z",
     "start_time": "2022-05-05T11:13:51.773727Z"
    }
   },
   "outputs": [],
   "source": [
    "# namedtuple은 key와 index를 통해 값에 접근할 수 있다.\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "\n",
    "#state: np[], action: tensor[[]], reward: [], next_state: np[], done: []\n",
    "\n",
    "# ReplayMemory를 정의\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        # deque는 양방향 queue를 의미한다.\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Transition을 저장, action은 이미 tensor[[]] 형태이므로 따로 전환 X\n",
    "        state = torch.tensor([state], device=device, dtype =torch.float)        \n",
    "        reward = torch.tensor([reward], device=device, dtype = torch.float)\n",
    "        next_state = torch.tensor([next_state], device=device, dtype =torch.float)\n",
    "        done =  torch.tensor([done], device=device, dtype = torch.bool)       \n",
    "        \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # memory로부터 batch_size 길이 만큼의 list를 반환한다.\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        # memory의 길이를 반환한다.\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41539d",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51b7e16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T11:13:51.947003Z",
     "start_time": "2022-05-05T11:13:51.932584Z"
    }
   },
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, action_size):\n",
    "        super(net, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8469b3da",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c315a0e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T11:13:52.105575Z",
     "start_time": "2022-05-05T11:13:52.090571Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        self.render = False\n",
    "\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.train_start = 1000\n",
    "\n",
    "        self.model = net(action_size).to(device)\n",
    "        self.target_model = net(action_size).to(device)\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.model.parameters(), self.learning_rate)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return torch.tensor([[random.randrange(self.action_size)]], device=device,\n",
    "                                dtype=torch.long)  # tensor[[]]\n",
    "        else:\n",
    "            state = torch.tensor([state], device=device, dtype=torch.float)\n",
    "            return self.model(state).detach().max(1)[1].view(1,1)  # tensor[[]]\n",
    "\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        transitions = memory.sample(BATCH_SIZE)\n",
    "        batch = Transition(*zip(*transitions))  # batch에 BATCH_SIZE 만큼 state_batch, 등 5종류를 구분하여 꺼내둠       \n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        next_state_batch = torch.cat(batch.next_state)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        done_batch = torch.cat(batch.done)\n",
    "\n",
    "        predicts = self.model(state_batch).gather(1, action_batch)\n",
    "        target_predicts = self.target_model(next_state_batch).detach()\n",
    "\n",
    "        max_q = target_predicts.max(1)[0]\n",
    "        targets = reward_batch + (~done_batch)*self.discount_factor*max_q\n",
    "\n",
    "    # Huber Loss 계산\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(predicts, targets.unsqueeze(1))  # unsqueeze(): 차원 추가\n",
    "\n",
    "    # Optimize parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.model.parameters():\n",
    "            # 모든 원소를 [ min, max ]의 범위로 clamp\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4f552",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ead4e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T11:13:53.029047Z",
     "start_time": "2022-05-05T11:13:52.255626Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "action_size = env.action_space.n\n",
    "memory = ReplayMemory(2000)\n",
    "\n",
    "agent = DQNAgent(action_size)\n",
    "scores, episodes = [], []\n",
    "score_avg=0\n",
    "HM_EPISODES = 300\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f972bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T11:16:09.162699Z",
     "start_time": "2022-05-05T11:13:53.173108Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode:   0 | score avg: 21.00 | steps: 21 | memory lengh:   21\n",
      "episode:   1 | score avg: 20.50 | steps: 16 | memory lengh:   37\n",
      "episode:   2 | score avg: 22.15 | steps: 37 | memory lengh:   74\n",
      "episode:   3 | score avg: 22.44 | steps: 25 | memory lengh:   99\n",
      "episode:   4 | score avg: 23.39 | steps: 32 | memory lengh:  131\n",
      "episode:   5 | score avg: 23.15 | steps: 21 | memory lengh:  152\n",
      "episode:   6 | score avg: 22.24 | steps: 14 | memory lengh:  166\n",
      "episode:   7 | score avg: 21.71 | steps: 17 | memory lengh:  183\n",
      "episode:   8 | score avg: 20.94 | steps: 14 | memory lengh:  197\n",
      "episode:   9 | score avg: 20.95 | steps: 21 | memory lengh:  218\n",
      "episode:  10 | score avg: 20.55 | steps: 17 | memory lengh:  235\n",
      "episode:  11 | score avg: 22.60 | steps: 41 | memory lengh:  276\n",
      "episode:  12 | score avg: 21.24 | steps:  9 | memory lengh:  285\n",
      "episode:  13 | score avg: 23.01 | steps: 39 | memory lengh:  324\n",
      "episode:  14 | score avg: 22.91 | steps: 22 | memory lengh:  346\n",
      "episode:  15 | score avg: 21.52 | steps:  9 | memory lengh:  355\n",
      "episode:  16 | score avg: 22.37 | steps: 30 | memory lengh:  385\n",
      "episode:  17 | score avg: 21.53 | steps: 14 | memory lengh:  399\n",
      "episode:  18 | score avg: 22.68 | steps: 33 | memory lengh:  432\n",
      "episode:  19 | score avg: 24.11 | steps: 37 | memory lengh:  469\n",
      "episode:  20 | score avg: 23.60 | steps: 19 | memory lengh:  488\n",
      "episode:  21 | score avg: 22.84 | steps: 16 | memory lengh:  504\n",
      "episode:  22 | score avg: 22.16 | steps: 16 | memory lengh:  520\n",
      "episode:  23 | score avg: 22.64 | steps: 27 | memory lengh:  547\n",
      "episode:  24 | score avg: 27.88 | steps: 75 | memory lengh:  622\n",
      "episode:  25 | score avg: 27.79 | steps: 27 | memory lengh:  649\n",
      "episode:  26 | score avg: 25.91 | steps:  9 | memory lengh:  658\n",
      "episode:  27 | score avg: 26.72 | steps: 34 | memory lengh:  692\n",
      "episode:  28 | score avg: 25.55 | steps: 15 | memory lengh:  707\n",
      "episode:  29 | score avg: 24.79 | steps: 18 | memory lengh:  725\n",
      "episode:  30 | score avg: 24.01 | steps: 17 | memory lengh:  742\n",
      "episode:  31 | score avg: 23.01 | steps: 14 | memory lengh:  756\n",
      "episode:  32 | score avg: 22.01 | steps: 13 | memory lengh:  769\n",
      "episode:  33 | score avg: 21.71 | steps: 19 | memory lengh:  788\n",
      "episode:  34 | score avg: 22.24 | steps: 27 | memory lengh:  815\n",
      "episode:  35 | score avg: 22.21 | steps: 22 | memory lengh:  837\n",
      "episode:  36 | score avg: 21.69 | steps: 17 | memory lengh:  854\n",
      "episode:  37 | score avg: 21.22 | steps: 17 | memory lengh:  871\n",
      "episode:  38 | score avg: 20.80 | steps: 17 | memory lengh:  888\n",
      "episode:  39 | score avg: 23.62 | steps: 49 | memory lengh:  937\n",
      "episode:  40 | score avg: 22.66 | steps: 14 | memory lengh:  951\n",
      "episode:  41 | score avg: 22.59 | steps: 22 | memory lengh:  973\n",
      "episode:  42 | score avg: 21.93 | steps: 16 | memory lengh:  989\n",
      "episode:  43 | score avg: 20.64 | steps:  9 | memory lengh:  998\n",
      "episode:  44 | score avg: 20.48 | steps: 19 | memory lengh: 1017\n",
      "episode:  45 | score avg: 19.53 | steps: 11 | memory lengh: 1028\n",
      "episode:  46 | score avg: 21.28 | steps: 37 | memory lengh: 1065\n",
      "episode:  47 | score avg: 21.65 | steps: 25 | memory lengh: 1090\n",
      "episode:  48 | score avg: 24.18 | steps: 47 | memory lengh: 1137\n",
      "episode:  49 | score avg: 22.87 | steps: 11 | memory lengh: 1148\n",
      "episode:  50 | score avg: 22.08 | steps: 15 | memory lengh: 1163\n",
      "episode:  51 | score avg: 24.07 | steps: 42 | memory lengh: 1205\n",
      "episode:  52 | score avg: 25.06 | steps: 34 | memory lengh: 1239\n",
      "episode:  53 | score avg: 25.46 | steps: 29 | memory lengh: 1268\n",
      "episode:  54 | score avg: 28.21 | steps: 53 | memory lengh: 1321\n",
      "episode:  55 | score avg: 31.19 | steps: 58 | memory lengh: 1379\n",
      "episode:  56 | score avg: 29.37 | steps: 13 | memory lengh: 1392\n",
      "episode:  57 | score avg: 28.23 | steps: 18 | memory lengh: 1410\n",
      "episode:  58 | score avg: 26.71 | steps: 13 | memory lengh: 1423\n",
      "episode:  59 | score avg: 30.04 | steps: 60 | memory lengh: 1483\n",
      "episode:  60 | score avg: 31.34 | steps: 43 | memory lengh: 1526\n",
      "episode:  61 | score avg: 30.30 | steps: 21 | memory lengh: 1547\n",
      "episode:  62 | score avg: 31.77 | steps: 45 | memory lengh: 1592\n",
      "episode:  63 | score avg: 31.19 | steps: 26 | memory lengh: 1618\n",
      "episode:  64 | score avg: 31.18 | steps: 31 | memory lengh: 1649\n",
      "episode:  65 | score avg: 33.96 | steps: 59 | memory lengh: 1708\n",
      "episode:  66 | score avg: 54.36 | steps:238 | memory lengh: 1946\n",
      "episode:  67 | score avg: 57.43 | steps: 85 | memory lengh: 2000\n",
      "episode:  68 | score avg: 70.88 | steps:192 | memory lengh: 2000\n",
      "episode:  69 | score avg: 81.09 | steps:173 | memory lengh: 2000\n",
      "episode:  70 | score avg: 76.59 | steps: 36 | memory lengh: 2000\n",
      "episode:  71 | score avg: 82.43 | steps:135 | memory lengh: 2000\n",
      "episode:  72 | score avg: 98.68 | steps:245 | memory lengh: 2000\n",
      "episode:  73 | score avg: 103.42 | steps:146 | memory lengh: 2000\n",
      "episode:  74 | score avg: 101.97 | steps: 89 | memory lengh: 2000\n",
      "episode:  75 | score avg: 107.48 | steps:157 | memory lengh: 2000\n",
      "episode:  76 | score avg: 109.73 | steps:130 | memory lengh: 2000\n",
      "episode:  77 | score avg: 108.46 | steps: 97 | memory lengh: 2000\n",
      "episode:  78 | score avg: 114.31 | steps:167 | memory lengh: 2000\n",
      "episode:  79 | score avg: 113.98 | steps:111 | memory lengh: 2000\n",
      "episode:  80 | score avg: 113.68 | steps:111 | memory lengh: 2000\n",
      "episode:  81 | score avg: 112.51 | steps:102 | memory lengh: 2000\n",
      "episode:  82 | score avg: 113.86 | steps:126 | memory lengh: 2000\n",
      "episode:  83 | score avg: 114.98 | steps:125 | memory lengh: 2000\n",
      "episode:  84 | score avg: 117.08 | steps:136 | memory lengh: 2000\n",
      "episode:  85 | score avg: 121.17 | steps:158 | memory lengh: 2000\n",
      "episode:  86 | score avg: 123.05 | steps:140 | memory lengh: 2000\n",
      "episode:  87 | score avg: 126.05 | steps:153 | memory lengh: 2000\n",
      "episode:  88 | score avg: 145.34 | steps:319 | memory lengh: 2000\n",
      "episode:  89 | score avg: 163.91 | steps:331 | memory lengh: 2000\n",
      "episode:  90 | score avg: 181.42 | steps:339 | memory lengh: 2000\n",
      "episode:  91 | score avg: 182.78 | steps:195 | memory lengh: 2000\n",
      "episode:  92 | score avg: 214.50 | steps:500 | memory lengh: 2000\n",
      "episode:  93 | score avg: 235.85 | steps:428 | memory lengh: 2000\n",
      "episode:  94 | score avg: 223.16 | steps:109 | memory lengh: 2000\n",
      "episode:  95 | score avg: 220.95 | steps:201 | memory lengh: 2000\n",
      "episode:  96 | score avg: 230.35 | steps:315 | memory lengh: 2000\n",
      "episode:  97 | score avg: 213.92 | steps: 66 | memory lengh: 2000\n",
      "episode:  98 | score avg: 237.03 | steps:445 | memory lengh: 2000\n",
      "episode:  99 | score avg: 241.02 | steps:277 | memory lengh: 2000\n",
      "episode: 100 | score avg: 241.82 | steps:249 | memory lengh: 2000\n",
      "episode: 101 | score avg: 252.84 | steps:352 | memory lengh: 2000\n",
      "episode: 102 | score avg: 255.25 | steps:277 | memory lengh: 2000\n",
      "episode: 103 | score avg: 257.23 | steps:275 | memory lengh: 2000\n",
      "episode: 104 | score avg: 255.91 | steps:244 | memory lengh: 2000\n",
      "episode: 105 | score avg: 253.92 | steps:236 | memory lengh: 2000\n",
      "episode: 106 | score avg: 262.02 | steps:335 | memory lengh: 2000\n",
      "episode: 107 | score avg: 259.02 | steps:232 | memory lengh: 2000\n",
      "episode: 108 | score avg: 254.02 | steps:209 | memory lengh: 2000\n",
      "episode: 109 | score avg: 251.32 | steps:227 | memory lengh: 2000\n",
      "episode: 110 | score avg: 246.99 | steps:208 | memory lengh: 2000\n",
      "episode: 111 | score avg: 243.19 | steps:209 | memory lengh: 2000\n",
      "episode: 112 | score avg: 241.07 | steps:222 | memory lengh: 2000\n",
      "episode: 113 | score avg: 237.36 | steps:204 | memory lengh: 2000\n",
      "episode: 114 | score avg: 233.83 | steps:202 | memory lengh: 2000\n",
      "episode: 115 | score avg: 230.34 | steps:199 | memory lengh: 2000\n",
      "episode: 116 | score avg: 225.51 | steps:182 | memory lengh: 2000\n",
      "episode: 117 | score avg: 222.76 | steps:198 | memory lengh: 2000\n",
      "episode: 118 | score avg: 219.38 | steps:189 | memory lengh: 2000\n",
      "episode: 119 | score avg: 215.04 | steps:176 | memory lengh: 2000\n",
      "episode: 120 | score avg: 212.24 | steps:187 | memory lengh: 2000\n",
      "episode: 121 | score avg: 212.12 | steps:211 | memory lengh: 2000\n",
      "episode: 122 | score avg: 211.80 | steps:209 | memory lengh: 2000\n",
      "episode: 123 | score avg: 210.02 | steps:194 | memory lengh: 2000\n",
      "episode: 124 | score avg: 207.62 | steps:186 | memory lengh: 2000\n",
      "episode: 125 | score avg: 204.76 | steps:179 | memory lengh: 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 126 | score avg: 201.88 | steps:176 | memory lengh: 2000\n",
      "episode: 127 | score avg: 200.79 | steps:191 | memory lengh: 2000\n",
      "episode: 128 | score avg: 200.32 | steps:196 | memory lengh: 2000\n",
      "episode: 129 | score avg: 199.38 | steps:191 | memory lengh: 2000\n",
      "episode: 130 | score avg: 200.05 | steps:206 | memory lengh: 2000\n",
      "episode: 131 | score avg: 198.34 | steps:183 | memory lengh: 2000\n",
      "episode: 132 | score avg: 196.11 | steps:176 | memory lengh: 2000\n",
      "episode: 133 | score avg: 196.80 | steps:203 | memory lengh: 2000\n",
      "episode: 134 | score avg: 196.92 | steps:198 | memory lengh: 2000\n",
      "episode: 135 | score avg: 197.52 | steps:203 | memory lengh: 2000\n",
      "episode: 136 | score avg: 196.37 | steps:186 | memory lengh: 2000\n",
      "episode: 137 | score avg: 197.24 | steps:205 | memory lengh: 2000\n",
      "episode: 138 | score avg: 199.01 | steps:215 | memory lengh: 2000\n",
      "episode: 139 | score avg: 200.91 | steps:218 | memory lengh: 2000\n",
      "episode: 140 | score avg: 203.32 | steps:225 | memory lengh: 2000\n",
      "episode: 141 | score avg: 208.69 | steps:257 | memory lengh: 2000\n",
      "episode: 142 | score avg: 214.62 | steps:268 | memory lengh: 2000\n",
      "episode: 143 | score avg: 231.06 | steps:379 | memory lengh: 2000\n",
      "episode: 144 | score avg: 257.95 | steps:500 | memory lengh: 2000\n",
      "episode: 145 | score avg: 267.06 | steps:349 | memory lengh: 2000\n",
      "episode: 146 | score avg: 290.35 | steps:500 | memory lengh: 2000\n",
      "episode: 147 | score avg: 311.32 | steps:500 | memory lengh: 2000\n",
      "episode: 148 | score avg: 324.78 | steps:446 | memory lengh: 2000\n",
      "episode: 149 | score avg: 342.31 | steps:500 | memory lengh: 2000\n",
      "episode: 150 | score avg: 358.07 | steps:500 | memory lengh: 2000\n",
      "episode: 151 | score avg: 365.47 | steps:432 | memory lengh: 2000\n",
      "episode: 152 | score avg: 378.92 | steps:500 | memory lengh: 2000\n",
      "episode: 153 | score avg: 358.33 | steps:173 | memory lengh: 2000\n",
      "episode: 154 | score avg: 372.50 | steps:500 | memory lengh: 2000\n",
      "episode: 155 | score avg: 360.65 | steps:254 | memory lengh: 2000\n",
      "episode: 156 | score avg: 374.58 | steps:500 | memory lengh: 2000\n",
      "episode: 157 | score avg: 387.12 | steps:500 | memory lengh: 2000\n",
      "episode: 158 | score avg: 398.41 | steps:500 | memory lengh: 2000\n",
      "episode: 159 | score avg: 408.57 | steps:500 | memory lengh: 2000\n",
      "episode: 160 | score avg: 417.71 | steps:500 | memory lengh: 2000\n",
      "episode: 161 | score avg: 425.94 | steps:500 | memory lengh: 2000\n",
      "episode: 162 | score avg: 433.35 | steps:500 | memory lengh: 2000\n",
      "episode: 163 | score avg: 440.01 | steps:500 | memory lengh: 2000\n",
      "episode: 164 | score avg: 446.01 | steps:500 | memory lengh: 2000\n",
      "episode: 165 | score avg: 451.41 | steps:500 | memory lengh: 2000\n",
      "episode: 166 | score avg: 456.27 | steps:500 | memory lengh: 2000\n",
      "episode: 167 | score avg: 460.64 | steps:500 | memory lengh: 2000\n",
      "episode: 168 | score avg: 464.58 | steps:500 | memory lengh: 2000\n",
      "episode: 169 | score avg: 468.12 | steps:500 | memory lengh: 2000\n",
      "episode: 170 | score avg: 471.31 | steps:500 | memory lengh: 2000\n",
      "episode: 171 | score avg: 474.18 | steps:500 | memory lengh: 2000\n",
      "episode: 172 | score avg: 476.76 | steps:500 | memory lengh: 2000\n",
      "episode: 173 | score avg: 479.08 | steps:500 | memory lengh: 2000\n",
      "episode: 174 | score avg: 481.18 | steps:500 | memory lengh: 2000\n",
      "episode: 175 | score avg: 483.06 | steps:500 | memory lengh: 2000\n",
      "episode: 176 | score avg: 484.75 | steps:500 | memory lengh: 2000\n",
      "episode: 177 | score avg: 467.78 | steps:315 | memory lengh: 2000\n",
      "episode: 178 | score avg: 446.40 | steps:254 | memory lengh: 2000\n",
      "episode: 179 | score avg: 418.96 | steps:172 | memory lengh: 2000\n",
      "episode: 180 | score avg: 427.06 | steps:500 | memory lengh: 2000\n",
      "episode: 181 | score avg: 434.36 | steps:500 | memory lengh: 2000\n",
      "episode: 182 | score avg: 440.92 | steps:500 | memory lengh: 2000\n",
      "episode: 183 | score avg: 446.83 | steps:500 | memory lengh: 2000\n",
      "episode: 184 | score avg: 439.95 | steps:378 | memory lengh: 2000\n",
      "episode: 185 | score avg: 445.95 | steps:500 | memory lengh: 2000\n",
      "episode: 186 | score avg: 451.36 | steps:500 | memory lengh: 2000\n",
      "episode: 187 | score avg: 456.22 | steps:500 | memory lengh: 2000\n",
      "episode: 188 | score avg: 460.60 | steps:500 | memory lengh: 2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11332\\2176739207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# (policy network에서) 최적화 한단계 수행\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# 다음 상태로 이동\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11332\\2414289240.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;31m# 모든 원소를 [ min, max ]의 범위로 clamp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                    )\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEHCAYAAABBW1qbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAf2ElEQVR4nO3deZgddZ3v8fcnAULYjUkgQ4DEe1kMo7L0MCKKbBcBZREFAurNw0R5VBBwgQkiAypcUGTuFQZQVB6iA0JckKAgYFSYQRQ6EIGwSJQtEJKA7EvI8r1/VJ0+p0/36a5Od52qc87n9Tznqapfbd9T3V3f/tWv6leKCMzMzABGFR2AmZmVh5OCmZn1cFIwM7MeTgpmZtbDScHMzHo4KZiZWY918ty4pMeAl4HVwKqI6JI0DrgGmAI8BhwZEc+ny58GzEyXPzEibhpo++PHj48pU6bkFb6ZWVuaP3/+sxExob95uSaF1N4R8WzN9CxgXkScJ2lWOv2vkqYB04EdgX8AfiNpu4hY3WjDU6ZMobu7O8/YzczajqTHG80r4vLRocDsdHw2cFhN+dURsSIiHgUWAbs1Pzwzs86Vd1II4GZJ8yUdl5ZtHhFLANLhxLR8S+DJmnUXp2VmZtYkeV8+2iMinpY0EbhF0kMDLKt+yvr0wZEml+MAtt5665GJ0szMgJxrChHxdDpcBlxLcjloqaRJAOlwWbr4YmCrmtUnA0/3s83LIqIrIromTOi3ncTMzNZSbklB0oaSNq6MA/sD9wNzgRnpYjOA69LxucB0SWMkTQW2Be7MKz4zM+srz8tHmwPXSqrs56qI+LWku4A5kmYCTwBHAETEQklzgAeAVcDxA915ZGZmIy+3pBARfwPe1U/5c8C+DdY5Bzgnr5jMzGxgfqLZzCxnU6eCBLffXnQkg2vGw2tmZh1pgw3g9der0+97H6xZU1w8WbimYGY2wiZNSmoGtQkBoBVedOmkYGY2Qj7zmSQZPPNMtezYY1sjGVT48pGZ2QgYNar3yf/tb4cHHui73O23wx57NC+uoXJNwcxsGG68MakdVBJCZby/hADwwQ82L7a14aRgZraWdt4ZDjqoOn300Y0bknffPRm++GL+cQ2HLx+Zma2FsWPhjTeq04O1G/zhD0ktouycFMzMhqj+5N5KDcmD8eUjM7MhqE0I6623dgnhggtGLp6R5qRgZpZRbUKYPBlWrBja+uuk12a++tXhxfGnP8GSJcPbRiNOCmZmGYweXR3fckt48snGyzay667J8OWX1z6Ohx6CAw9Mnn/Ig5OCmdkgxo+v3lW04YawePHabWfevOHFcf/9SVcZ664Ll146vG014oZmM7MBfOtb8Nxz1elXXln7bW244dqv+9pryUNvL70E112XdLKXB9cUzMwGcMop1fGRvMvo1VeHtvy0aUlC+Oxn4ZBDRi6Oek4KZmYN1DYsj/RtpwcfnH3ZI46Axx+HLbaAiy8e2TjqOSmYmfVjk02q4+eeO3Lb3WijZPhf/9V4mV12qZ78f/5z+OlPkwR1770jF0cjTgpmZnWWLKneISTBrFkjt+0vfjEZrlrV//z99oN77oETToCVK5NaAsC3vw0TJoxcHI0oWvhRvK6uruju7i46DDNrM3leNqrdfv22n3wStt66Oj16NKxeDRMnwtKlI7l/zY+Irv7muaZgZlZjypTq+C9/2dx9V+4oGj8+Ga5enQwffrh5MTgpmJnVePzx6ngzu7n+8perSWD5cnjnO5Pxww+HzTZrXhxOCmZmqbwvG9W78cbqeKUx+8gjk+GCBfDoo/Czn+UfRy0nBTOzOltske/2K8nnS19KhjvuWJ13zTXVZWovZTWLk4KZGb1rCXl1NldRaTNYtCgZVt7S9r3v5bvfLJwUzKzjPfVUdXz69Pz394lPJMM334RJk5JxCT75yfz3PRgnBTPreJMnV8d//OP89/f1r1fHn3kmGf7qV/nvNwsnBTOz1AknNGc/G2zQt+zAA5uz78E4KZhZR6ttS7joomJiuPrqYvbbHycFMzNgn32K2/dRRxW373pOCmbWsWprCcN9Ac5QjR2bDE88sbn7HYxfsmNmHW84L79ZWy+/DKNG9U5MZeCkYGYdaa+9quPDeZva2qp953OZ+PKRmXWkW28tOoJyclIws472xBNFR1AuuScFSaMl3SPpl+n0OEm3SHokHb6lZtnTJC2S9LCkD+Qdm5l1ptrr+FttVVwcZdSMmsJJwIM107OAeRGxLTAvnUbSNGA6sCNwAHCJpJJedTOzdtDMLqlbRa5JQdJk4IPA92uKDwVmp+OzgcNqyq+OiBUR8SiwCNgtz/jMrPOcfXZ1/Pnni4ujrPKuKfw/4FRgTU3Z5hGxBCAdTkzLtwSerFlucVrWi6TjJHVL6l6+fHkuQZtZ+zrjjKIjKLfckoKkDwHLImJ+1lX6KevzmouIuCwiuiKia0Iz3mJtZm3prLOKjqCc8nxOYQ/gEEkHAesDm0j6T2CppEkRsUTSJGBZuvxioLbJZzLwdI7xmVmHqX295plnFhdHmeVWU4iI0yJickRMIWlA/m1EfByYC8xIF5sBXJeOzwWmSxojaSqwLXBnXvGZWee54YaiIyi/Ip5oPg+YI2km8ARwBEBELJQ0B3gAWAUcHxGrC4jPzNrchRcWHUF5KZrxduqcdHV1RXd3d9FhmFkLeP/74bbbkvEWPu2NCEnzI6Krv3l+otnMOkIlIdjAnBTMrKN87nNFR1BuTgpm1hauvz7pvmKjjfrO+/3vq+NuTxiYk4KZtYVDDkmGr77ad97eezc3llbmpGBmHWPcuKIjKD8nBTNrS1Lft5o991wxsbQSJwUza3mj6s5ktW81K+JVm63MScHMWl79cwdrarrgfO215sbS6pwUzKwjXHFF0RG0BicFM2tpte0GX/964+VmzGg8z6qcFMysLYweDV/5StFRtD4nBTNrWbW1hFWr+s5fb71kuMkmzYmnHTgpmFnLW3fdvmX33gsrViSN0C++2PyYWlURXWebmY2oN9+sjnd6D6jD5ZqCmbWko48uOoL25KRgZi3p6quLjqA9OSmYWUur78rChsdJwcxaWu3TyzZ8Tgpm1nJ+8YuiI2hfTgpm1nI+/OGiI2hfTgpmZtbDScHMWta11xYdQftxUjCzlnXYYUVH0H4yJQVJYyVtn3cwZmaD2WqroiNob4MmBUkHAwuAX6fTO0mam3NcZmb9Wry46AjaW5aawlnAbsALABGxAJiSV0BmZllUekC1kZUlKayKCPcxaGalsmJF0RG0pyy9pN4v6RhgtKRtgROBP+QblplZXzvvXHQE7S9LTeFzwI7ACuAq4EXg5BxjMjPr14IFRUfQ/gasKUgaDcyNiP2A05sTkpnZwHbaqegI2teANYWIWA28JmnTJsVjZjaoe+4pOoL2laVN4Q3gPkm3AK9WCiPixNyiMjOr4y6ymyNLUvhV+jEzszY3aFKIiNmS1gO2S4sejoiVg60naX3gNmBMup+fRsSZksYB15A86/AYcGREPJ+ucxowE1gNnBgRNw35G5lZW9tnn6IjaG9ZnmjeC3gEuBi4BPiLpD0zbHsFsE9EvAvYCThA0ruBWcC8iNgWmJdOI2kaMJ3kTqcDgEvShm4zsx7z5hUdQXvLckvqBcD+EfH+iNgT+ADwfwdbKRKvpJPrpp8ADgVmp+WzgcPS8UOBqyNiRUQ8CiwieZLazMyaJEtSWDciHq5MRMRfSE7wg5I0WtICYBlwS0T8Cdg8Ipak21oCTEwX3xJ4smb1xWmZmXW4sWOLjqBzZGlo7pb0A+BH6fTHgPlZNp7e0rqTpM2AayX94wCL93dvQfRZSDoOOA5g6623zhKGmbW4N94oOoLOkaWm8BlgIUn3FicBDwCfHspOIuIF4PckbQVLJU0CSIfL0sUWA7Wd4k4Gnu5nW5dFRFdEdE2YMGEoYZiZ2SCyJIV1gG9HxOER8WHgQmDQBmBJE9IaApLGAvsBDwFzgRnpYjOA69LxucB0SWMkTQW2Be4cwncxsza3cGHREbS/LJeP5pGc0CuNxmOBm4H3DLLeJGB2egfRKGBORPxS0h3AHEkzgSeAIwAiYqGkOSQ1kVXA8enlJzMzAKZNKzqC9pclKaxfcxcREfGKpA0GWyki7gX69GkYEc8B+zZY5xzgnAwxmVmHmDOn6Ag6S5bLR69K2qUyIWlX4PX8QjIzqzrqqKIj6CxZagonAz+RVGn0nQT4x2Rm1oaydHNxl6QdgO1Jbht9KEs3F2ZmI2m0+zdoiizdXBxB0q5wP8lTx9fUXk4yM2uGVauKjqAzZGlTOCMiXpb0XpIuLmYDl+YblpmZFSFLUqjcFvpB4NKIuA5YL7+QzMwSfi6h+bIkhackfRc4ErhB0piM65mZDcs/DtQxjuUiy8n9SOAm4IC0u4pxwCl5BmVmZsXIcvfRa8DPa6aXAEvyDMrMzIrhy0BmJbdyJUSf/oLb3/33V8c78fsXJcvDa2ZWoPVqbuvopJPjO95RdASdKVNNQdI2kvZLx8dK2jjfsMw61+qabiBV95aR+ulO0NVVdASdJcvDa58Cfgp8Ny2aDPwix5jMOpKUfNZZpzreaLl2t/761fG77ioujk6UpaZwPLAH8BJARDxC9RWaZjZMM2cOfqKvv2y0Xps/KbRiRdERdK4sSWFFRLxZmZC0Dv28JtPM1s7ll/eebtRuUFu+cmVn1Bis+bIkhVslfRkYK+l/AT8Brs83LLPOUHti//Snqyf+iOTz3vfCmjXVZWrH69dvR7u4l7WmUwxyO4OkUcBMYH+SXlJvAr4fg63YBF1dXdHd3V10GGZrrfakPpS/qPpkUPxf48iqfL92+15lIWl+RPTbhJ/l4bU1wPfSj5nlYN9+30XYWETvxCD5BGojY9CkIOk++rYhvAh0A2enr9c0syF4/nkYN646/ZvfDH0bTgyWhywPr91I0lPqVen09HT4EnAFcPDIh2XWvkayHaC/xPDud8Mdd4zcPpptzJiiI+hsWZLCHhGxR830fZJuj4g9JH08r8DM2lF/CeGNN4a3zQh461vh739Ppv/4R5g4EZYtG952i/Lmm4MvY/nJcvfRRpL+uTIhaTdgo3TS70IyqzHQQ2f1KncYjcR/xs891/vS0fLl7X9nkuUjS03hk8DlkjYiufvoJeCTkjYEzs0zOLNWNdj1/byu/bdTO8MOOxQdQWfKcvfRXcA7JG1KcgvrCzWz5+QVmFmrueKK/subfXtluySGBx8sOoLOlKmXVEkfBHYE1lf62xYRX8sxLrOWc+yxfcu22KI63szLOfWJYcKE5JKS2WCydIj3HeAo4HMkl4+OALbJOS6zlrfddrB0aXH7r60dPPss/PrXxcWSVbv36dQKsjQ0vyci/jfwfER8Fdgd2CrfsMxa3yOPFB1B78Rw4IHlb3xeubLoCCxLUqjcMPeapH8AVgJT8wvJrL3ttVdz93d9XU9lZU8MAJtvXnQEnStLm8L1kjYDzgfuJnm62V1emDVQfz2/3u9+17xYAD70odZrfH7mmaIj6FwD1hTSzvDmRcQLEfEzkraEHSLi35oSnVmLmDCh93TlGYRddinPybc+jrLVGA46qOgIDAZJCmlneBfUTK+IiBdzj8qsxTz7bP/l8+c3N47BlDkx3Hhj0REYZGtTuFnSR6Qy/fqY2drqLzHUfsaPb35MP/xhdXz06Obv36qyJIUvkLxY501JL0l6WdJLOcdl1lbWrIFzzy3vpaRazz2XJIczz2xePDNmVMdXufOcQg2aFCJi44gYFRHrRsQm6fQmzQjOrNXUv1qzQoJZs5oby2AiYPXqJGFFwLx5ved/7WvNubxUu48FC/Lfnw0sy8NrkvRxSWek01ulneINtt5Wkn4n6UFJCyWdlJaPk3SLpEfS4Vtq1jlN0iJJD0v6wHC+mFkR+nuqucxGjaqelPfZJ0kOv/1t72XyTAyf/Wzv6Xe9K799WTZZLh9dQvLA2jHp9CvAxRnWWwV8MSLeDrwbOF7SNGAWyR1N2wLz0mnSedNJutM4ALhEkq8umjXZ3ns3bncYSRJceml1uiyX1jpdlqTwzxFxPOlDbBHxPDDow+gRsSQi7k7HXwYeBLYEDgVmp4vNBg5Lxw8Frk7vcHoUWAQMWiMxs3xEwKab9i6rb5SufCZPzr7d9dfvm2Bef3348drIyJIUVqb/sQeApAnAmqHsRNIUYGfgT8DmEbEEksQBTEwX2xJ4sma1xWlZ/baOk9QtqXu5e/iyEmjn+/JeeCHbf/BPPZUch+nTGyeOymfFit7rRiSJwsohS1K4ELgWmCjpHOC/gf+TdQfpexh+BpwcEQPdtdTfn1afX8eIuCwiuiKia0L9E0NmlovKw3iVTyPXXDP0bVq5ZHmfwpWS5gP7kpy4D4uITD2dS1qXJCFcGRE/T4uXSpoUEUskTQIqLw1cTO+O9iYDT2f8HmbWRPUn8zvugPe8J9u6a9a0d+2q1WW5++jbwLiIuDgi/mMICUHAD4AHI+Lfa2bNBSp3Jc8Arqspny5pjKSpwLbAnRm/h5kVaPfd+9YmGn2cEMotS4d4dwNfkbQdyWWkayKiO8N6ewCfAO6TtCAt+zJwHjBH0kzgCZL3MxARCyXNAR4guXPp+IhYPZQvY1akrP8pm5WZIuNFPUnjgI+Q3Da6dXpLaaG6urqiuztLfjLLT7Nft2k2XJLmR0RXf/OyNDRX/E9gB2AK8NAIxGVmZiWTpU3hG5IeAb4GLAR2jYiDc4/MrOTyeKDLrGhZ2hQeBXaPiAadA5uZWbvIckvqdyS9Je3vaP2a8ttyjcysRbgtwdrJoElB0ieBk0ieG1hA0o/RHcA+uUZmVmK+bGTtKktD80nAPwGPR8TeJN1VuH8JM7M2lCUpvBERbwBIGhMRDwHb5xuWmZkVIUtD82JJmwG/AG6R9DzufsIMcHuCtZ8sDc0fTkfPkvQ7YFPg17lGZVZi3/9+0RGY5SdLTaFHRNyaVyBmreJTnyo6ArP8DOWJZjMza3NOCmZm1sNJwWwtuZHZ2pGTgtkQnHpq0RGY5ctJwWwIzj+/6AjM8uWkYGZmPZwUzNbC+usPvoxZK3JSMFsLr79edARm+XBSMMvoqKOKjsAsf04KZhnNmVN0BGb5c1IwM7MeTgpmQ7TJJkVHYJYfJwWzIXrxxaIjMMuPk4JZBh/7WNERmDWHk4JZBlddVXQEZs3hpGA2CKnoCMyax0nBrIGLLuqbEC69tJhYzJplSG9eM+sUXV0wf37vMneVbZ3AScGszq67wt139y5zQrBO4ctH1lGk5HPhhY3n1SaECCcE6yxOCtYxatsHTjqpmgQqn3oXX9y82MzKwpePrO1JcPDBQ1vHtQPrVK4pWNuqrQFcf321vHJJqPbS0Dbb9C0z60S5JQVJl0taJun+mrJxkm6R9Eg6fEvNvNMkLZL0sKQP5BWXdYahPFsQAY89llsoZi0lz5rCFcABdWWzgHkRsS0wL51G0jRgOrBjus4lkkbnGJu1oW99q//2gcp//6tWuRZgNpjckkJE3Ab8va74UGB2Oj4bOKym/OqIWBERjwKLgN3yis3a0ymn9C2rTQKj/W+G2aCa3aaweUQsAUiHE9PyLYEna5ZbnJaZZeKuKMxGRlkamvv7k+63oi/pOEndkrqXL1+ec1jWitxgbLb2mp0UlkqaBJAOl6Xli4GtapabDDzd3wYi4rKI6IqIrgkTJuQarLWG7bevjjsRmA1Ps5PCXGBGOj4DuK6mfLqkMZKmAtsCdzY5NmtRf/lL0RGYtY/cHl6T9GNgL2C8pMXAmcB5wBxJM4EngCMAImKhpDnAA8Aq4PiIWJ1XbGZm1r/ckkJEHN1g1r4Nlj8HOCeveKz9ffObRUdg1vrK0tBstlbOP7863t8tqWY2NE4K1nJefRVWrEjGTz212FjM2o07xLOWs9FGRUdg1r5cU7CWct55/Zf7VlSzkeGkYC3ltNOKjsCsvTkpWMtzLcFs5LhNwVpGbf9GTgRm+XBNwczMejgpmJlZDycFazm+dGSWHycFawl+X4JZczgpmJlZDycFK73aWsLZZxcXh1kncFKwUqu/bHT66cXEYdYpnBSsZbiB2Sx/TgrWEpwQzJrDScHMzHo4KVhp+TZUs+ZzUjAzsx5OCmZm1sNJwUpvypSiIzDrHE4KVnqPPlp0BGadw0nBcrN69cDzI2DNmmT8jDPgxBOr89zIbFYMv2THclF/Uq99zmCgE/5FF+UTj5ll45pCyUi9PxWt8vDWbbf1f9Lv7ztl0Srf26xdOCmUXOVEOmrU2p9YR1J/l4ROPbUa1/vfn31bEXDmmdXx/j5m1lxOCsCqVX3/Q89yAj7kkOr8+nUOP7xaXrluPtA+hnqyr19v5szqvNWrk++UZb2s+60st846fdc9//y+y48f3/vkvsUWvedXTvhnneWTv1mZKFr4L7Krqyu6u7uHvF4rNGJWfiwvvwwbblitKbSClSuT5GFm5SRpfkR09TfPNYUCRMCttw48v2LjjZOEUCmv/Rx7bL5x9ucLX+hbVh+XE4JZ6+rIpFBfOWp0PTvL9e3KvNde67v8mjWNt7PnnsO/jn755dljv/LKwb9vI5//fHWZCy7wdX+zdtax/9OtzclsoHXGju1bVqbLPccck3wG4hO8mXVkTcHMzPrnpGBmZj2cFMzMrEfpkoKkAyQ9LGmRpFlFx2Nm1klKlRQkjQYuBg4EpgFHS5pWbFRmZp2jVEkB2A1YFBF/i4g3gauBQwuOycysY5QtKWwJPFkzvTgt6yHpOEndkrqXL1/e1ODMzNpd2ZJCf3f297p7PiIui4iuiOiaMGFCk8IyM+sMZXt4bTGwVc30ZODpRgvPnz//WUmPD2N/44Fnh7F+3hzf8JU9xrLHB+WPsezxQfli3KbRjFJ1iCdpHeAvwL7AU8BdwDERsTCn/XU36hSqDBzf8JU9xrLHB+WPsezxQWvEWFGqmkJErJJ0AnATMBq4PK+EYGZmfZUqKQBExA3ADUXHYWbWicrW0NxslxUdwCAc3/CVPcayxwflj7Hs8UFrxAiUrE3BzMyK1ek1BTMzq9GRSaFs/StJ2krS7yQ9KGmhpJPS8rMkPSVpQfo5qOA4H5N0XxpLd1o2TtItkh5Jh28pKLbta47TAkkvSTq56GMo6XJJyyTdX1PW8JhJOi39vXxY0gcKiu98SQ9JulfStZI2S8unSHq95lh+J+/4Boix4c+1JMfwmprYHpO0IC0v5BgOSUR01Ifkrqa/Am8D1gP+DEwrOKZJwC7p+MYkt+VOA84CvlT0MauJ8zFgfF3ZN4FZ6fgs4BsliHM08AzJvdiFHkNgT2AX4P7Bjln6M/8zMAaYmv6eji4gvv2BddLxb9TEN6V2uYKPYb8/17Icw7r5FwD/VuQxHMqnE2sKpetfKSKWRMTd6fjLwIPUde9RYocCs9Px2cBhxYXSY1/grxExnAcbR0RE3Ab8va640TE7FLg6IlZExKPAIpLf16bGFxE3R8SqdPKPJA+RFqbBMWykFMewQpKAI4Ef5xnDSOrEpDBo/0pFkjQF2Bn4U1p0QlqNv7yoSzM1ArhZ0nxJx6Vlm0fEEkiSGzCxsOiqptP7j7BMxxAaH7My/m7+C3BjzfRUSfdIulXS+4oKKtXfz7Vsx/B9wNKIeKSmrEzHsI9OTAqD9q9UFEkbAT8DTo6Il4BLgf8B7AQsIamGFmmPiNiFpGvz4yXtWXA8fUhaDzgE+ElaVLZjOJBS/W5KOh1YBVyZFi0Bto6InYEvAFdJ2qSg8Br9XEt1DIGj6f0PSpmOYb86MSkMqX+lZpG0LklCuDIifg4QEUsjYnVErAG+R87V4MFExNPpcBlwbRrPUkmTANLhsuIiBJKEdXdELIXyHcNUo2NWmt9NSTOADwEfi/RieHpJ5rl0fD7J9frtiohvgJ9rmY7hOsDhwDWVsjIdw0Y6MSncBWwraWr6X+V0YG6RAaXXHX8APBgR/15TPqlmsQ8D99ev2yySNpS0cWWcpDHyfpJjNyNdbAZwXTER9uj1n1mZjmGNRsdsLjBd0hhJU4FtgTubHZykA4B/BQ6JiNdqyicoeREWkt6Wxve3ZseX7r/Rz7UUxzC1H/BQRCyuFJTpGDZUdEt3ER/gIJI7fP4KnF6CeN5LUsW9F1iQfg4CfgTcl5bPBSYVGOPbSO7q+DOwsHLcgLcC84BH0uG4AmPcAHgO2LSmrNBjSJKglgArSf6LnTnQMQNOT38vHwYOLCi+RSTX5Su/i99Jl/1I+rP/M3A3cHCBx7Dhz7UMxzAtvwL4dN2yhRzDoXz8RLOZmfXoxMtHZmbWgJOCmZn1cFIwM7MeTgpmZtbDScHMzHo4KZgNg6SvSdpvBLbzykjEYzZcviXVrAQkvRIRGxUdh5lrCmZ1JH1c0p1pf/fflTRa0iuSLpB0t6R5kiaky14h6aPp+HmSHkg7aftWWrZNuvy96XDrtHyqpDsk3SXp63X7PyUtv1fSV5v9/a2zOSmY1ZD0duAoks7/dgJWAx8DNiTpU2kX4FbgzLr1xpF0t7BjRLwTODud9R/AD9OyK4EL0/JvA5dGxD+RvPuhsp39Sbo+2I2ks7ddy9jxoLUvJwWz3vYFdgXuSt+WtS9JFx9rqHZs9p8kXZPUegl4A/i+pMOBSp9BuwNXpeM/qllvD6p9NP2oZjv7p597SLpB2IEkSZg1xTpFB2BWMgJmR8RpvQqlM+qW69UYFxGrJO1GkkSmAycA+/Sz/WgwXrv/cyPiu0MN3GwkuKZg1ts84KOSJkLP+5S3Iflb+Wi6zDHAf9eulL4LY9OIuAE4meTSD8AfSJIEJJehKuvdXldecRPwL+n2kLRlJRazZnBNwaxGRDwg6Sskb5gbRdLz5fHAq8COkuYDL5K0O9TaGLhO0vok/+1/Pi0/Ebhc0inAcuDYtPwkkhesnETyHo3K/m9O2zXuSHpU5xXg4xT/ngrrEL4l1SwD3zJqncKXj8zMrIdrCmZm1sM1BTMz6+GkYGZmPZwUzMysh5OCmZn1cFIwM7MeTgpmZtbj/wO9DBCcJ95aGgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for episode in range(HM_EPISODES):    \n",
    "    done = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    \n",
    "    # env와 state 초기화\n",
    "    state = env.reset()         \n",
    "\n",
    "    while not done:    \n",
    "        \n",
    "        if agent.render:\n",
    "            env.render()\n",
    "        \n",
    "        action = agent.get_action(state)    # action: tensor[[]]\n",
    "\n",
    "        next_state, reward, done, info = env.step(action.item()) #next_state: np[], reward: [], done: []               \n",
    "        score += reward\n",
    "        reward = 0.1 if not done or score == 500 else -1        \n",
    "        \n",
    "        memory.push(state, action, next_state, reward, done) #state: np[], action: tensor[[]], reward: [], next_state: np[], done: []\n",
    "\n",
    "        # (policy network에서) 최적화 한단계 수행        \n",
    "        if len(memory) >= agent.train_start:\n",
    "            agent.train_model()\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        # 마찬가지로 done이 True 라면,\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            score_avg = 0.9 * score_avg + 0.1 * score if score_avg !=0 else score\n",
    "            print('episode: {:3d} | score avg: {:3.2f} | steps:{:3d} | memory lengh: {:4d}'.format(\n",
    "                episode, score_avg, steps, len(memory)))\n",
    "            scores.append(score_avg)\n",
    "            episodes.append(episode)\n",
    "            plt.plot(episodes, scores, 'b')\n",
    "            plt.xlabel('episode')\n",
    "            plt.ylabel('average score')\n",
    "            \n",
    "   \n",
    "            if score_avg > 1000:\n",
    "                sys.exit()       \n",
    "    \n",
    "\n",
    "    \n",
    "print('Complete')\n",
    "scores.clear\n",
    "episodes.clear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2bb72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a30220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a0c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
