{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3661219c",
   "metadata": {},
   "source": [
    "# https://yjs-program.tistory.com/173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df042e61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T11:09:48.452565Z",
     "start_time": "2022-05-02T11:09:48.435611Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym # 카트폴같은 여러 게임 환경 제공 패키지\n",
    "# pip install gym 으로 설치 가능#\n",
    "import random # 에이전트가 무작위로 행동할 확률을 구하기 위함.\n",
    "import math # 에이전트가 무작위로 행동할 확률을 구하기 위함.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "#Deque : 먼저 들어온 데이터가 먼저 나가는 FIFO 자료구조의 일종으로 double-ended queue의 약자로, 일반적인 큐와 달리 양쪽 끝에서 삽입,삭제가 모두 가능한 자료구조다.\n",
    "import matplotlib.pyplot as plt\n",
    "# 필수 모듈 임포트하기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e208ef22",
   "metadata": {},
   "source": [
    "# DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c5e8b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T11:09:59.369368Z",
     "start_time": "2022-05-02T11:09:59.347427Z"
    }
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 256),#신경망은 카트 위치, 카트 속도, 막대기 각도, 막대기 속도까지 4가지 정보를 입력\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2)#왼쪽으로 갈 때의 가치와 오른쪽으로 갈 때의 가치\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
    "        self.steps_done = 0 #self.steps_done은 학습을 반복할 때마다 증가\n",
    "        self.memory = deque(maxlen=10000)#memory에 deque사용.(큐가 가득 차면 제일 오래된것부터 삭제)\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state):#에피소드 저장 함수\n",
    "        # self.memory = [(상태, 행동, 보상, 다음 상태)...]\n",
    "        self.memory.append((state,\n",
    "                            action,\n",
    "                            torch.FloatTensor([reward]),\n",
    "                            torch.FloatTensor([next_state])))\n",
    "    \n",
    "    def act(self, state):#행동 선택(담당)함수\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1\n",
    "        #무작위 숫자와 엡실론을 비교\n",
    "\n",
    "        # 엡실론 그리디(Epsilon-Greedy) \n",
    "        # : 초기엔 엡실론을 높게=최대한 경험을 많이 하도록 / 엡실론을 낮게 낮춰가며 = 신경망이 선택하는 비율 상승\n",
    "        if random.random() > eps_threshold:# 무작위 값 > 앱실론값 : 학습된 신경망이 옳다고 생각하는 쪽으로,\n",
    "            return self.model(state).data.max(1)[1].view(1, 1)\n",
    "        else:# 무작위 값 < 앱실론값 : 무작위로 행동\n",
    "            return torch.LongTensor([[random.randrange(2)]])\n",
    "    \n",
    "    def learn(self):#메모리에 쌓아둔 경험들을 재학습(replay)하며, 학습하는 함수\n",
    "        if len(self.memory) < BATCH_SIZE:# 메모리에 저장된 에피소드가 batch 크기보다 작으면 그냥 학습을 거름.\n",
    "            return\n",
    "        #경험이 충분히 쌓일 때부터 학습 진행\n",
    "        batch = random.sample(self.memory, BATCH_SIZE) # 메모리에서 무작위로 Batch 크기만큼 가져와서 학습\n",
    "        states, actions, rewards, next_states = zip(*batch) #기존의 batch를 요소별 리스트로 분리해줄 수 있게끔\n",
    "\n",
    "        #리스트를 Tensor형태로\n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "\n",
    "        # 모델의 입력으로 states를 제공, 현 상태에서 했던 행동의 가치(Q값)을 current_q로 모음\n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        \n",
    "        max_next_q = self.model(next_states).detach().max(1)[0]#에이전트가 보는 행동의 미래 가치(max_next_q)\n",
    "        expected_q = rewards + (GAMMA * max_next_q)# rewards(보상)+미래가치\n",
    "        \n",
    "        # 행동은 expected_q를 따라가게끔, MSE_loss로 오차 계산, 역전파, 신경망 학습\n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4f414",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be9cf31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T11:10:05.193791Z",
     "start_time": "2022-05-02T11:10:05.178831Z"
    }
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 정의\n",
    "EPISODES = 100   # 애피소드(총 플레이할 게임 수) 반복횟수\n",
    "EPS_START = 0.9  # 학습 시작시 에이전트가 무작위로 행동할 확률\n",
    "# ex) 0.5면 50% 절반의 확률로 무작위 행동, 나머지 절반은 학습된 방향으로 행동\n",
    "# random하게 EPisolon을 두는 이유는 Agent가 가능한 모든 행동을 경험하기 위함.\n",
    "EPS_END = 0.05   # 학습 막바지에 에이전트가 무작위로 행동할 확률\n",
    "#EPS_START에서 END까지 점진적으로 감소시켜줌.\n",
    "# --> 초반에는 경험을 많이 쌓게 하고, 점차 학습하면서 똑똑해지니깐 학습한대로 진행하게끔\n",
    "EPS_DECAY = 200  # 학습 진행시 에이전트가 무작위로 행동할 확률을 감소시키는 값\n",
    "GAMMA = 0.8      # 할인계수 : 에이전트가 현재 reward를 미래 reward보다 얼마나 더 가치있게 여기는지에 대한 값.\n",
    "# 일종의 할인율\n",
    "LR = 0.001       # 학습률\n",
    "BATCH_SIZE = 64  # 배치 크기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2127313f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T11:10:08.216706Z",
     "start_time": "2022-05-02T11:10:08.194765Z"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')# env : 게임 환경 / 원하는 게임을 make함수에 넣어주면 됨.\n",
    "agent = DQNAgent()\n",
    "score_history = [] #점수 저장용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2a3472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T11:09:27.345017Z",
     "start_time": "2022-05-02T11:07:38.970864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드:1 점수: 17\n",
      "에피소드:2 점수: 28\n",
      "에피소드:3 점수: 18\n",
      "에피소드:4 점수: 13\n",
      "에피소드:5 점수: 21\n",
      "에피소드:6 점수: 11\n",
      "에피소드:7 점수: 11\n",
      "에피소드:8 점수: 9\n",
      "에피소드:9 점수: 19\n",
      "에피소드:10 점수: 18\n",
      "에피소드:11 점수: 12\n",
      "에피소드:12 점수: 10\n",
      "에피소드:13 점수: 12\n",
      "에피소드:14 점수: 11\n",
      "에피소드:15 점수: 12\n",
      "에피소드:16 점수: 19\n",
      "에피소드:17 점수: 11\n",
      "에피소드:18 점수: 10\n",
      "에피소드:19 점수: 14\n",
      "에피소드:20 점수: 9\n",
      "에피소드:21 점수: 8\n",
      "에피소드:22 점수: 12\n",
      "에피소드:23 점수: 13\n",
      "에피소드:24 점수: 14\n",
      "에피소드:25 점수: 13\n",
      "에피소드:26 점수: 13\n",
      "에피소드:27 점수: 37\n",
      "에피소드:28 점수: 17\n",
      "에피소드:29 점수: 26\n",
      "에피소드:30 점수: 100\n",
      "에피소드:31 점수: 66\n",
      "에피소드:32 점수: 59\n",
      "에피소드:33 점수: 54\n",
      "에피소드:34 점수: 107\n",
      "에피소드:35 점수: 84\n",
      "에피소드:36 점수: 200\n",
      "에피소드:37 점수: 200\n",
      "에피소드:38 점수: 200\n",
      "에피소드:39 점수: 200\n",
      "에피소드:40 점수: 200\n",
      "에피소드:41 점수: 200\n",
      "에피소드:42 점수: 200\n",
      "에피소드:43 점수: 200\n",
      "에피소드:44 점수: 200\n",
      "에피소드:45 점수: 182\n",
      "에피소드:46 점수: 179\n",
      "에피소드:47 점수: 200\n",
      "에피소드:48 점수: 200\n",
      "에피소드:49 점수: 174\n",
      "에피소드:50 점수: 200\n",
      "에피소드:51 점수: 189\n",
      "에피소드:52 점수: 200\n",
      "에피소드:53 점수: 171\n",
      "에피소드:54 점수: 139\n",
      "에피소드:55 점수: 174\n",
      "에피소드:56 점수: 200\n",
      "에피소드:57 점수: 200\n",
      "에피소드:58 점수: 200\n",
      "에피소드:59 점수: 170\n",
      "에피소드:60 점수: 186\n",
      "에피소드:61 점수: 200\n",
      "에피소드:62 점수: 173\n",
      "에피소드:63 점수: 200\n",
      "에피소드:64 점수: 200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-289b50811961>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 게임이 끝날때까지 무한루프\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 현 상태를 Tensor화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(1, EPISODES+1):#50번의 플레이(EPISODE수 만큼)\n",
    "    state = env.reset() # 매 시작마다 환경 초기화\n",
    "    steps = 0\n",
    "    while True: # 게임이 끝날때까지 무한루프\n",
    "        env.render()\n",
    "        state = torch.FloatTensor([state]) # 현 상태를 Tensor화\n",
    "\n",
    "        # 에이전트의 act 함수의 입력으로 state 제공\n",
    "        # Epsilon Greedy에 따라 행동 선택\n",
    "        action = agent.act(state) \n",
    "        \n",
    "        # action : tensor, item 함수로 에이전트가 수행한 행동의 번호 추출\n",
    "        # step함수의 입력에 제공 ==> 다음 상태, reward, 종료 여부(done, Boolean Value) 출력\n",
    "        next_state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        # 게임이 끝났을 경우 마이너스 보상주기\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        agent.memorize(state, action, reward, next_state) # 경험(에피소드) 기억\n",
    "        agent.learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"에피소드:{0} 점수: {1}\".format(e, steps))\n",
    "            score_history.append(steps) #score history에 점수 저장\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7512f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bb106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
