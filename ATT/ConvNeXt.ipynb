{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1pLXm3_uYEmHiXOy6D8Lvgm0lHY3PqWA_","timestamp":1684389641917}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torchvision.ops.misc import Permute\n","from torchvision.ops import StochasticDepth"],"metadata":{"id":"ygUactBbDL4f","executionInfo":{"status":"ok","timestamp":1712930984338,"user_tz":-540,"elapsed":18453,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["class CNBlock(nn.Module):\n","    def __init__(self, in_channels, layer_scale, stochastic_depth_prob):\n","        super().__init__()\n","\n","        self.residual = nn.Sequential(nn.Conv2d(in_channels, in_channels, 7, padding=3, groups=in_channels), # torchvision 구현체 보면 bias=True 임. 이유는? 잘 모르겠음\n","                                      Permute([0, 2, 3, 1]), # 개채행열 -> 개행열채 # torch.permute() 를 쓰려면 forward 에서 해야하기 때문에 nn.Sequential() 안에 넣으려면 이렇게!\n","                                      nn.LayerNorm(in_channels, eps=1e-6), # pixel-wise로 채널 축의 값들을 이용해서 normalize 하는 것\n","                                      # nn.LayerNorm 은 주어진 차원의 마지막부터 시작하여 역순으로 정규화를 수행한다.\n","                                      # 예를 들어, x = torch.randn(개, 채, 행, 열) 라면 layer_norm = nn.LayerNorm([채, 행, 열]) 이렇게 리스트로 줘야 함\n","                                      # (위는 채,행,열 에 대해서 전부다 평균 구하는 데 참여시키다는 뜻)\n","                                      # [개, 행, 열] 이런 식으로 건너뛸 수 없다는 것!\n","                                      Permute([0, 3, 1, 2]), # 개행열채 -> 개채행열\n","                                      nn.Conv2d(in_channels, 4 * in_channels, 1),\n","                                      nn.GELU(),\n","                                      nn.Conv2d(4 * in_channels, in_channels, 1))\n","        self.layer_scale = nn.Parameter(torch.ones(1,in_channels, 1, 1) * layer_scale)\n","        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n","\n","    def forward(self, x):\n","        residual = self.layer_scale * self.residual(x) # 어떤 channel이 중요한지를 학습시키자 (SE Net 아이디어 비슷?)\n","        residual = self.stochastic_depth(residual)\n","        out = residual + x\n","        return out\n","\n","class ConvNeXt(nn.Module):\n","    def __init__(self, block_setting, stochastic_depth_prob = 0.0, layer_scale = 1e-6, num_classes = 1000, **kwargs):\n","        super().__init__()\n","\n","        layers = []\n","        layers += [nn.Sequential(nn.Conv2d(3, block_setting[0][0], kernel_size=4, stride=4),\n","                                 Permute([0, 2, 3, 1]),\n","                                 nn.LayerNorm(block_setting[0][0], eps=1e-6),\n","                                 Permute([0, 3, 1, 2]))]\n","\n","        total_stage_blocks = sum([setting[2] for setting in block_setting])\n","        stage_block_id = 0\n","        for in_channels, out_channels, num_blocks in block_setting:\n","            stage = []\n","            for _ in range(num_blocks):\n","                sd_prob = stochastic_depth_prob * stage_block_id / (total_stage_blocks - 1) # 1 빼야 마지막 블록이 설정한 stochastic_depth_prob을 가지게 된다.\n","                stage.append(CNBlock(in_channels, layer_scale, sd_prob))\n","                stage_block_id += 1\n","            layers += [nn.Sequential(*stage)]\n","            if out_channels is not None:\n","                downsample = nn.Sequential(Permute([0, 2, 3, 1]),\n","                                           nn.LayerNorm(in_channels),\n","                                           Permute([0, 3, 1, 2]),\n","                                           nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=2))\n","                layers += [downsample]\n","\n","        self.features = nn.Sequential(*layers)\n","\n","        # https://github.com/pytorch/vision/blob/main/torchvision/models/convnext.py#L160 참고\n","        # Swin과 달리 pre-activation이 아니라서 LN-GAP-fc 가 아님. 근데 GAP-fc 만 하기엔 마지막 LN이 멀어서 GAP-fc 사이에 LN을 추가한 듯\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.classifier = nn.Sequential(nn.LayerNorm(block_setting[-1][0]),\n","                                        nn.Linear(block_setting[-1][0], num_classes))\n","\n","        for m in self.modules():\n","            if isinstance(m, (nn.Conv2d, nn.Linear)):\n","                nn.init.trunc_normal_(m.weight, std=0.02) # 논문엔 0.2 라 나와있는데 torchvision 코드는 0.02로 되어있음. 참고: https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py#L603\n","                # timm 코드도 0.02 로 되어있어서 코드도 0.02로 반영. 참고: https://github.com/huggingface/pytorch-image-models/blob/4d9c3ae2fb7cc4739ec57d4c06254d2ffc7e2c89/timm/models/convnext.py#L380\n","                # head init scale 은 fine-tuning 때 하는 것이므로 여기선 생략. 적용 코드 참고: https://github.com/huggingface/pytorch-image-models/blob/4d9c3ae2fb7cc4739ec57d4c06254d2ffc7e2c89/timm/models/convnext.py#L383\n","                if m.bias is not None:\n","                    nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"t1n_VkKJC66-","executionInfo":{"status":"ok","timestamp":1712930984883,"user_tz":-540,"elapsed":549,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def ConvNeXt_T(**kwargs):\n","    block_setting = [[96, 192, 3],\n","                     [192, 384, 3],\n","                     [384, 768, 9],\n","                     [768, None, 3]]\n","    return ConvNeXt(block_setting, stochastic_depth_prob = 0.1,  **kwargs)\n","\n","def ConvNeXt_S(**kwargs):\n","    block_setting = [[96, 192, 3],\n","                     [192, 384, 3],\n","                     [384, 768, 27],\n","                     [768, None, 3]]\n","    return ConvNeXt(block_setting, stochastic_depth_prob = 0.4, **kwargs)\n","\n","def ConvNeXt_B(**kwargs):\n","    block_setting = [[128, 256, 3],\n","                     [256, 512, 3],\n","                     [512, 1024, 27],\n","                     [1024, None, 3]]\n","    return ConvNeXt(block_setting, stochastic_depth_prob = 0.5, **kwargs)\n","\n","def ConvNeXt_L(**kwargs):\n","    block_setting = [[192, 384, 3],\n","                     [384, 768, 3],\n","                     [768, 1536, 27],\n","                     [1536, None, 3]]\n","    return ConvNeXt(block_setting, stochastic_depth_prob = 0.5, **kwargs)"],"metadata":{"id":"Dg1kW2SVELbK","executionInfo":{"status":"ok","timestamp":1712930984884,"user_tz":-540,"elapsed":13,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["model = ConvNeXt_L()\n","# print(model)\n","!pip install torchinfo\n","from torchinfo import summary\n","summary(model, input_size=(2,3,224,224), device='cpu')"],"metadata":{"id":"PA2-KLar_wKa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712931008036,"user_tz":-540,"elapsed":23163,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"faf054f8-1942-4efa-d58b-bd793de6e81d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]},{"output_type":"execute_result","data":{"text/plain":["===============================================================================================\n","Layer (type:depth-idx)                        Output Shape              Param #\n","===============================================================================================\n","ConvNeXt                                      [2, 1000]                 --\n","├─Sequential: 1-1                             [2, 1536, 7, 7]           --\n","│    └─Sequential: 2-1                        [2, 192, 56, 56]          --\n","│    │    └─Conv2d: 3-1                       [2, 192, 56, 56]          9,408\n","│    │    └─Permute: 3-2                      [2, 56, 56, 192]          --\n","│    │    └─LayerNorm: 3-3                    [2, 56, 56, 192]          384\n","│    │    └─Permute: 3-4                      [2, 192, 56, 56]          --\n","│    └─Sequential: 2-2                        [2, 192, 56, 56]          --\n","│    │    └─CNBlock: 3-5                      [2, 192, 56, 56]          306,048\n","│    │    └─CNBlock: 3-6                      [2, 192, 56, 56]          306,048\n","│    │    └─CNBlock: 3-7                      [2, 192, 56, 56]          306,048\n","│    └─Sequential: 2-3                        [2, 384, 28, 28]          --\n","│    │    └─Permute: 3-8                      [2, 56, 56, 192]          --\n","│    │    └─LayerNorm: 3-9                    [2, 56, 56, 192]          384\n","│    │    └─Permute: 3-10                     [2, 192, 56, 56]          --\n","│    │    └─Conv2d: 3-11                      [2, 384, 28, 28]          295,296\n","│    └─Sequential: 2-4                        [2, 384, 28, 28]          --\n","│    │    └─CNBlock: 3-12                     [2, 384, 28, 28]          1,201,920\n","│    │    └─CNBlock: 3-13                     [2, 384, 28, 28]          1,201,920\n","│    │    └─CNBlock: 3-14                     [2, 384, 28, 28]          1,201,920\n","│    └─Sequential: 2-5                        [2, 768, 14, 14]          --\n","│    │    └─Permute: 3-15                     [2, 28, 28, 384]          --\n","│    │    └─LayerNorm: 3-16                   [2, 28, 28, 384]          768\n","│    │    └─Permute: 3-17                     [2, 384, 28, 28]          --\n","│    │    └─Conv2d: 3-18                      [2, 768, 14, 14]          1,180,416\n","│    └─Sequential: 2-6                        [2, 768, 14, 14]          --\n","│    │    └─CNBlock: 3-19                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-20                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-21                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-22                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-23                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-24                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-25                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-26                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-27                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-28                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-29                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-30                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-31                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-32                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-33                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-34                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-35                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-36                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-37                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-38                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-39                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-40                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-41                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-42                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-43                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-44                     [2, 768, 14, 14]          4,763,136\n","│    │    └─CNBlock: 3-45                     [2, 768, 14, 14]          4,763,136\n","│    └─Sequential: 2-7                        [2, 1536, 7, 7]           --\n","│    │    └─Permute: 3-46                     [2, 14, 14, 768]          --\n","│    │    └─LayerNorm: 3-47                   [2, 14, 14, 768]          1,536\n","│    │    └─Permute: 3-48                     [2, 768, 14, 14]          --\n","│    │    └─Conv2d: 3-49                      [2, 1536, 7, 7]           4,720,128\n","│    └─Sequential: 2-8                        [2, 1536, 7, 7]           --\n","│    │    └─CNBlock: 3-50                     [2, 1536, 7, 7]           18,963,456\n","│    │    └─CNBlock: 3-51                     [2, 1536, 7, 7]           18,963,456\n","│    │    └─CNBlock: 3-52                     [2, 1536, 7, 7]           18,963,456\n","├─AdaptiveAvgPool2d: 1-2                      [2, 1536, 1, 1]           --\n","├─Sequential: 1-3                             [2, 1000]                 --\n","│    └─LayerNorm: 2-9                         [2, 1536]                 3,072\n","│    └─Linear: 2-10                           [2, 1000]                 1,537,000\n","===============================================================================================\n","Total params: 197,767,336\n","Trainable params: 197,767,336\n","Non-trainable params: 0\n","Total mult-adds (G): 68.81\n","===============================================================================================\n","Input size (MB): 1.20\n","Forward/backward pass size (MB): 828.55\n","Params size (MB): 790.96\n","Estimated Total Size (MB): 1620.71\n","==============================================================================================="]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["x = torch.randn(2,3,224,224)\n","print(model(x).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-TNV93Y-_wVK","executionInfo":{"status":"ok","timestamp":1712931011698,"user_tz":-540,"elapsed":3675,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"af043355-fd00-4ad5-c6bd-0045bca5f597"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1000])\n"]}]},{"cell_type":"code","source":["# Layer scale 기법에 대해\n","class MyModel(nn.Module):\n","    def __init__(self, in_channels, layer_scale):\n","        super().__init__()\n","        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n","        self.layer_scale = nn.Parameter(torch.ones(1, in_channels, 1, 1) * layer_scale)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        print(out.shape)\n","        print(self.layer_scale.shape)\n","        print(self.layer_scale)\n","        print(out)\n","        out = out * self.layer_scale  # Apply layer scaling\n","        print(out)\n","        return out\n","\n","model = MyModel(in_channels=2, layer_scale=0.5)\n","y=model(torch.randn(1, 2, 4, 4))\n","print(y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xwhyyn7xrQVe","executionInfo":{"status":"ok","timestamp":1712931011699,"user_tz":-540,"elapsed":7,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"151ead48-f053-4c50-e6aa-58743d47fda7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 2, 4, 4])\n","torch.Size([1, 2, 1, 1])\n","Parameter containing:\n","tensor([[[[0.5000]],\n","\n","         [[0.5000]]]], requires_grad=True)\n","tensor([[[[ 0.1397,  0.0777, -0.0704, -0.1303],\n","          [ 0.0194, -0.4569, -0.4124, -0.5984],\n","          [ 0.5002,  0.5851,  0.5253,  0.5692],\n","          [ 0.2156,  0.6633, -0.5025, -0.0132]],\n","\n","         [[ 0.0898,  0.6209, -0.1042,  0.6007],\n","          [ 0.2597,  0.0625, -1.1589, -0.1244],\n","          [-0.2417, -0.1112, -0.0112,  0.1503],\n","          [ 0.3058, -0.3266, -0.0510,  0.2003]]]],\n","       grad_fn=<ConvolutionBackward0>)\n","tensor([[[[ 0.0699,  0.0388, -0.0352, -0.0651],\n","          [ 0.0097, -0.2285, -0.2062, -0.2992],\n","          [ 0.2501,  0.2925,  0.2626,  0.2846],\n","          [ 0.1078,  0.3316, -0.2512, -0.0066]],\n","\n","         [[ 0.0449,  0.3104, -0.0521,  0.3003],\n","          [ 0.1299,  0.0312, -0.5794, -0.0622],\n","          [-0.1208, -0.0556, -0.0056,  0.0751],\n","          [ 0.1529, -0.1633, -0.0255,  0.1002]]]], grad_fn=<MulBackward0>)\n","torch.Size([1, 2, 4, 4])\n"]}]},{"cell_type":"code","source":["y.sum().backward()\n","print(model.layer_scale.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KpG1l61Mkme","executionInfo":{"status":"ok","timestamp":1712931012251,"user_tz":-540,"elapsed":557,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"c8ad28fe-358e-4e2c-93e8-f85776e1dee9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[1.1114]],\n","\n","         [[0.1608]]]])\n"]}]},{"cell_type":"code","source":["# Layer norm, 각 픽셀 위치 마다, 채널 축에 대해서 하려면?\n","x = torch.randn(2, 3, 2, 2)\n","ln = nn.LayerNorm(3, eps=1e-5)\n","y = ln(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n","print(\"After LayerNorm: \", y) # 각 픽셀 위치에서 채널 축으로 평균 0 분산 1 이 되도록 함\n","print(\"weight shape: \", ln.weight.shape)"],"metadata":{"id":"6eHPmBAuuDMk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712931012252,"user_tz":-540,"elapsed":25,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"a46950bc-1a04-4bb6-c43f-e98c402c9ad4"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["After LayerNorm:  tensor([[[[ 0.9828,  1.2877],\n","          [ 1.4139, -1.4134]],\n","\n","         [[-1.3720, -0.1376],\n","          [-0.7306,  0.6678]],\n","\n","         [[ 0.3892, -1.1501],\n","          [-0.6834,  0.7456]]],\n","\n","\n","        [[[ 1.2521,  0.4983],\n","          [-1.3906,  1.2001]],\n","\n","         [[-0.0567, -1.3953],\n","          [ 0.4727,  0.0479]],\n","\n","         [[-1.1954,  0.8970],\n","          [ 0.9179, -1.2480]]]], grad_fn=<PermuteBackward0>)\n","weight shape:  torch.Size([3])\n"]}]},{"cell_type":"code","source":["# Layer norm, 각 픽셀 위치 마다, 채널 축에 대해서 하려면? (직접 구하기)\n","mean = x.mean(dim=1, keepdim=True)\n","std = x.std(dim=1, keepdim=True, unbiased=False)\n","print(mean.shape)\n","print(std.shape)\n","y_manual = (x - mean) / (std + 1e-5)\n","print(\"Manual normalization: \", y_manual)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_sKN-oY0wGhT","executionInfo":{"status":"ok","timestamp":1712931012252,"user_tz":-540,"elapsed":24,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"4c803b29-4069-4c5a-b44e-37b995c4a995"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1, 2, 2])\n","torch.Size([2, 1, 2, 2])\n","Manual normalization:  tensor([[[[ 0.9829,  1.2877],\n","          [ 1.4139, -1.4135]],\n","\n","         [[-1.3720, -0.1376],\n","          [-0.7306,  0.6678]],\n","\n","         [[ 0.3892, -1.1501],\n","          [-0.6834,  0.7456]]],\n","\n","\n","        [[[ 1.2521,  0.4983],\n","          [-1.3906,  1.2001]],\n","\n","         [[-0.0567, -1.3953],\n","          [ 0.4727,  0.0479]],\n","\n","         [[-1.1954,  0.8970],\n","          [ 0.9179, -1.2480]]]])\n"]}]},{"cell_type":"code","source":["# 배치놈은 어땟나\n","x = torch.randn(2, 3, 2, 2)\n","bn = nn.BatchNorm2d(3, eps=1e-5)\n","y_bn = bn(x)\n","print(\"After BatchNorm2d: \", y_bn)\n","print(\"weight shape: \", bn.weight.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzzqdBMf1qX0","executionInfo":{"status":"ok","timestamp":1712931012252,"user_tz":-540,"elapsed":22,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"3cab3465-7b87-4bf9-c14f-96b05269fc81"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["After BatchNorm2d:  tensor([[[[ 0.8974,  0.0792],\n","          [ 0.6630, -1.0971]],\n","\n","         [[ 0.2158, -0.8629],\n","          [ 0.6147, -0.2307]],\n","\n","         [[ 0.4788,  0.5985],\n","          [-1.6553,  1.2414]]],\n","\n","\n","        [[[-0.6904,  1.1693],\n","          [-1.7715,  0.7501]],\n","\n","         [[ 0.0380,  1.5227],\n","          [-1.9934,  0.6959]],\n","\n","         [[-1.2367,  0.1695],\n","          [ 1.0654, -0.6617]]]], grad_fn=<NativeBatchNormBackward0>)\n","weight shape:  torch.Size([3])\n"]}]},{"cell_type":"code","source":["# 배치놈은 어땟나 (직접 구하기)\n","mean = x.mean(dim=(0, 2, 3), keepdim=True)\n","std = x.std(dim=(0, 2, 3), keepdim=True, unbiased=False)\n","print(mean.shape)\n","print(std.shape)\n","y_manual = (x - mean) / (std + 1e-5)\n","print(\"Manual Batch Normalization: \", y_manual)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0b2KdwmvwZc","executionInfo":{"status":"ok","timestamp":1712931012252,"user_tz":-540,"elapsed":18,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"11d482c2-7eca-4603-c9f4-9f94c77e9732"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 3, 1, 1])\n","torch.Size([1, 3, 1, 1])\n","Manual Batch Normalization:  tensor([[[[ 0.8974,  0.0792],\n","          [ 0.6629, -1.0971]],\n","\n","         [[ 0.2158, -0.8629],\n","          [ 0.6147, -0.2307]],\n","\n","         [[ 0.4788,  0.5985],\n","          [-1.6553,  1.2414]]],\n","\n","\n","        [[[-0.6904,  1.1693],\n","          [-1.7715,  0.7501]],\n","\n","         [[ 0.0380,  1.5227],\n","          [-1.9934,  0.6959]],\n","\n","         [[-1.2367,  0.1695],\n","          [ 1.0654, -0.6617]]]])\n"]}]},{"cell_type":"code","source":["# 흔히 알려진 layer norm 그림에선..\n","x = torch.randn(2, 3, 2, 2)\n","ln = nn.LayerNorm([3, 2, 2], eps=1e-5, )\n","y = ln(x)\n","print(\"After LayerNorm: \", y)\n","print(\"weight shape: \", ln.weight.shape) # nn.LayerNorm([C,H,W]) 이면 C*H*W 개에 대해서 평균, 분산 내서 normalize 하는 거고 재배치할 평균, 분산도 각각 CHW 개다"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x6RuYrolg2F5","executionInfo":{"status":"ok","timestamp":1712931012252,"user_tz":-540,"elapsed":15,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"ff2539fa-784e-41d5-c999-a47b497ef931"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["After LayerNorm:  tensor([[[[-0.2695,  0.7893],\n","          [-0.0237, -0.3577]],\n","\n","         [[-1.4171, -1.6256],\n","          [ 1.4453, -0.0731]],\n","\n","         [[-0.1290, -0.8005],\n","          [ 1.8404,  0.6214]]],\n","\n","\n","        [[[ 1.2495, -1.5074],\n","          [ 0.7325,  1.2575]],\n","\n","         [[ 0.8298,  0.8966],\n","          [-1.4235,  0.0346]],\n","\n","         [[-0.0081, -1.4134],\n","          [-0.7245,  0.0763]]]], grad_fn=<NativeLayerNormBackward0>)\n","weight shape:  torch.Size([3, 2, 2])\n"]}]},{"cell_type":"code","source":["# 흔히 알려진 layer norm 그림에선.. (직접 구하기)\n","mean = x.mean(dim=(1, 2, 3), keepdim=True)\n","std = x.std(dim=(1, 2, 3), keepdim=True, unbiased=False)\n","print(mean.shape)\n","print(std.shape)\n","y_manual = (x - mean) / (std + 1e-5)\n","print(\"Manual normalization: \", y_manual)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o9NIPFcuvj-M","executionInfo":{"status":"ok","timestamp":1712931012253,"user_tz":-540,"elapsed":13,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"c9b31f9f-d4c0-454b-9ce6-8d1ca84fb9d7"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 1, 1, 1])\n","torch.Size([2, 1, 1, 1])\n","Manual normalization:  tensor([[[[-0.2695,  0.7893],\n","          [-0.0237, -0.3577]],\n","\n","         [[-1.4171, -1.6256],\n","          [ 1.4453, -0.0731]],\n","\n","         [[-0.1290, -0.8005],\n","          [ 1.8404,  0.6214]]],\n","\n","\n","        [[[ 1.2495, -1.5074],\n","          [ 0.7325,  1.2575]],\n","\n","         [[ 0.8298,  0.8966],\n","          [-1.4234,  0.0346]],\n","\n","         [[-0.0081, -1.4134],\n","          [-0.7245,  0.0763]]]])\n"]}]},{"cell_type":"code","source":["# 내 필기용\n","from functools import partial\n","\n","def power(base, exponent):\n","    return base ** exponent\n","\n","square = partial(power, 2)  # 밑을 2로 고정\n","cube = partial(power, 3)  # 밑을 3으로 고정\n","\n","print(square(4))  # 출력: 16 (2의 4제곱)\n","print(cube(3))  # 출력: 27 (3의 3제곱)"],"metadata":{"id":"qy87RVMxKoJk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1712931012253,"user_tz":-540,"elapsed":12,"user":{"displayName":"ppen hyuk","userId":"06590780498642875598"}},"outputId":"c2c946c2-4102-4a1c-efa2-1ff804395a92"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["16\n","27\n"]}]}]}